{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks with Keras\n",
    "\n",
    "Note: Please watch the video for this NoteBook, explaning how Keras works will help you build your own Neural Networks! \n",
    "\n",
    "Here we are using Keras library to implement Neural Networks. Keras is a deep learning API which runs on top of TensorFlow. You can find more information about Keras and TensorFlow in the following links:\n",
    "\n",
    "https://keras.io/about/\n",
    "\n",
    "https://www.tensorflow.org\n",
    "\n",
    "Here, we would like to make a credit card fraud detection model. This was another Kaggle competition; the data is huge so you have to make an account and take the file from Kaggle ;) \n",
    "\n",
    "https://www.kaggle.com/mlg-ulb/creditcardfraud/. \n",
    "\n",
    "The dataset presents here is a two-day credit card transactions. Out of 284,807 transactions, only 492 of them are fraud. Thus the data is highly skewed (unbalanced dataset). All the features in this example are numerical, but for most of the feaures we dont know the names and only some alias of V1 to V28. However, we do know name of two features. There is a feature called \"Time\" and another which is called \"Amount\". Time presents the time elapsed (in unit of seconds) between two transactions and the \"Amount\" is the amount of each transaction. \n",
    "\n",
    "\n",
    "Note: Data is uploaded in OWL under the resources/ Unit 5 video files Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries that we need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the cell above we imported the libraries we needed from which we have already talked about pandas in details. Numpy might be a new library for you! NumPy is a scientific computing package of Python designed for fast operations on arrays (including mathematical and logical processes). Moreover, it has basic linear algebra and statistical operation helper functions. The well-written documentation of Numpy includes simple and usefull examples:\n",
    "https://numpy.org/doc/stable/user/whatisnumpy.html\n",
    "\n",
    "- Sklearn has a data preprocessing section too! https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing it is highly recommanded to take a look at all its options. \n",
    "\n",
    "- As mentioned earlier Keras is implemented on top of TensorFlow and can be easily be called from tensorflow. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "data = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Classification task, this means it is a supervised method (have features and targets). Here, the target is a binary (fraud or not fraud) label. We are going to make the target and features dataframe, and we are going to make a numpy array (instead of pandas data frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make target and feature seperate from each other  \n",
    "target = data[\"Class\"]\n",
    "features = data.drop(columns = [\"Class\"])\n",
    "# change the data frame to be a numpy array\n",
    "target = np.array(target)\n",
    "features = np.array(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have a training set and a validation set (train the model and see how robust it is using the validation set). We assign 20% of data as validation and the rest for training. We used a sklearn helper function in the previous codes to split our data; here we show another method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the length of 20% of the data\n",
    "num_val_samples = int(len(features) * 0.2)\n",
    "\n",
    "# split data into train and validation\n",
    "train_features = features[:-num_val_samples]\n",
    "train_target = target[:-num_val_samples]\n",
    "val_features = features[-num_val_samples:]\n",
    "val_target = target[-num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced data: \n",
    "\n",
    "- Our data-set is highly skewed, we want to see in our data-set how many of the cased are positive (fraudulent) and how many are negative (law-full transactions). So, we can just look at our \"train_target\" array and count all 0s and 1s (numpy provides an easy way to do so).\n",
    "\n",
    "- As, it is important to be able to detect as many positive (fraudulent) cases as possible it is a good practice to give the positive cases more weight! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_0 = len(np.where(train_target ==0)[0])\n",
    "count_1 = len(np.where(train_target ==1)[0])\n",
    "\n",
    "weight_for_0 = 1.0 / count_0\n",
    "weight_for_1 = 1.0 / count_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of data:\n",
    "\n",
    "It is a good practice to scale the data. For many algorithms the estimators might behave poorly if the features are not scaled. When the features have different ranges for example one feature varies between 0 and 1 and the other between 0 and 100000; the algorithm might put more weight (significance) on the larger numbers however, they are not more important! There are different methods to scale a data-set. One commen practice is to standardize features by removing the mean and scaling to unit variance. Python provides a comprehensive set of options:\n",
    "https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\n",
    "\n",
    "In this notebook, we are goona use the StandardScaler method which is explained in the link below: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# We \"fit\" and \"transform\" our data as follows:\n",
    "train_feature_normal = scaler.fit_transform(train_features)\n",
    "val_features_normal = scaler.fit_transform(val_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sequential Class:\n",
    "\n",
    "We are using Sequential class within Keras. If you recall from Lesson 1, Neural Networks contain linear layers which are connected to each other via a nonlinear (activation) function.  Sequential groups a linear stack of layers and build a model. To build each layer, we can use Keras Layers API (Keras.layers) and we use a regular densly connected network (keras.layers.Dense) where we can define which activation function we want to use (exe: relu, tanh, and many more options) and we can define how many neurons each layer should have Input layer has the shape of the length of features; and the outout layer has the shape of output which for the binary classfication is 1. Layers between the input and output layers (the hidden layers) can have what ever number of neurons we choose (a hyperparameter choice). Between each layer we put anothe layer of Dropput. This step is not necessary (although recommended) and it randomly sets some input values to the new layer to 0; this helps preventing overfitting! \n",
    "\n",
    "Below is a set of useful links to learn more about Keras layers. \n",
    "- Sequential explained: https://keras.io/api/models/sequential/\n",
    "- Layer explained: https://keras.io/api/layers/\n",
    "- Dense Layer explained: https://keras.io/api/layers/core_layers/dense/\n",
    "- Dropout Layer: https://keras.io/api/layers/regularization_layers/dropout/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               7936      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 139,777\n",
      "Trainable params: 139,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(\n",
    "            256, activation=\"relu\", input_shape=(train_features.shape[-1],)\n",
    "        ),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(256, activation=\"relu\"),\n",
    "        keras.layers.Dropout(0.3),\n",
    "        keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the training begin!\n",
    "\n",
    "- This is a classification task so to evaluate the perfomance we should take a look at False Negatives, False Positives, True Negatives, True Positives, Precision and Recall. So, we can use Keras.metric to calculate the above metrics! \n",
    "\n",
    "- After building the model; we need to train the model which is simple using Keras! model.compile and model.fit are the only things we need to use! In model.compile we choose what loss function we want to use, which optimization method and ...\n",
    "\n",
    "- model.fit will have many features: train features and labels, batch-size, epoch and many more! \n",
    "- We made sure to put the class weight as a parameter (remember we want to put more weight to the posotive cases as they are rare but critical to catch!) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "112/112 - 15s - loss: 2.1961e-06 - fn: 45.0000 - fp: 27388.0000 - tn: 200041.0000 - tp: 372.0000 - precision: 0.0134 - recall: 0.8921 - val_loss: 0.1471 - val_fn: 6.0000 - val_fp: 2588.0000 - val_tn: 54298.0000 - val_tp: 69.0000 - val_precision: 0.0260 - val_recall: 0.9200\n",
      "Epoch 2/30\n",
      "112/112 - 9s - loss: 1.3889e-06 - fn: 32.0000 - fp: 6687.0000 - tn: 220742.0000 - tp: 385.0000 - precision: 0.0544 - recall: 0.9233 - val_loss: 0.1923 - val_fn: 6.0000 - val_fp: 3782.0000 - val_tn: 53104.0000 - val_tp: 69.0000 - val_precision: 0.0179 - val_recall: 0.9200\n",
      "Epoch 3/30\n",
      "112/112 - 8s - loss: 1.1753e-06 - fn: 28.0000 - fp: 7480.0000 - tn: 219949.0000 - tp: 389.0000 - precision: 0.0494 - recall: 0.9329 - val_loss: 0.2308 - val_fn: 3.0000 - val_fp: 5154.0000 - val_tn: 51732.0000 - val_tp: 72.0000 - val_precision: 0.0138 - val_recall: 0.9600\n",
      "Epoch 4/30\n",
      "112/112 - 8s - loss: 9.8716e-07 - fn: 24.0000 - fp: 6267.0000 - tn: 221162.0000 - tp: 393.0000 - precision: 0.0590 - recall: 0.9424 - val_loss: 0.1467 - val_fn: 6.0000 - val_fp: 3523.0000 - val_tn: 53363.0000 - val_tp: 69.0000 - val_precision: 0.0192 - val_recall: 0.9200\n",
      "Epoch 5/30\n",
      "112/112 - 7s - loss: 8.9119e-07 - fn: 20.0000 - fp: 7449.0000 - tn: 219980.0000 - tp: 397.0000 - precision: 0.0506 - recall: 0.9520 - val_loss: 0.0807 - val_fn: 8.0000 - val_fp: 1842.0000 - val_tn: 55044.0000 - val_tp: 67.0000 - val_precision: 0.0351 - val_recall: 0.8933\n",
      "Epoch 6/30\n",
      "112/112 - 7s - loss: 8.3055e-07 - fn: 22.0000 - fp: 8298.0000 - tn: 219131.0000 - tp: 395.0000 - precision: 0.0454 - recall: 0.9472 - val_loss: 0.1421 - val_fn: 5.0000 - val_fp: 3921.0000 - val_tn: 52965.0000 - val_tp: 70.0000 - val_precision: 0.0175 - val_recall: 0.9333\n",
      "Epoch 7/30\n",
      "112/112 - 8s - loss: 7.6767e-07 - fn: 18.0000 - fp: 5997.0000 - tn: 221432.0000 - tp: 399.0000 - precision: 0.0624 - recall: 0.9568 - val_loss: 0.1425 - val_fn: 7.0000 - val_fp: 3354.0000 - val_tn: 53532.0000 - val_tp: 68.0000 - val_precision: 0.0199 - val_recall: 0.9067\n",
      "Epoch 8/30\n",
      "112/112 - 7s - loss: 1.1054e-06 - fn: 15.0000 - fp: 7377.0000 - tn: 220052.0000 - tp: 402.0000 - precision: 0.0517 - recall: 0.9640 - val_loss: 1.0177 - val_fn: 0.0000e+00 - val_fp: 10607.0000 - val_tn: 46279.0000 - val_tp: 75.0000 - val_precision: 0.0070 - val_recall: 1.0000\n",
      "Epoch 9/30\n",
      "112/112 - 7s - loss: 1.8573e-06 - fn: 27.0000 - fp: 7879.0000 - tn: 219550.0000 - tp: 390.0000 - precision: 0.0472 - recall: 0.9353 - val_loss: 0.1218 - val_fn: 7.0000 - val_fp: 2118.0000 - val_tn: 54768.0000 - val_tp: 68.0000 - val_precision: 0.0311 - val_recall: 0.9067\n",
      "Epoch 10/30\n",
      "112/112 - 8s - loss: 8.5927e-07 - fn: 19.0000 - fp: 5528.0000 - tn: 221901.0000 - tp: 398.0000 - precision: 0.0672 - recall: 0.9544 - val_loss: 0.1338 - val_fn: 8.0000 - val_fp: 2603.0000 - val_tn: 54283.0000 - val_tp: 67.0000 - val_precision: 0.0251 - val_recall: 0.8933\n",
      "Epoch 11/30\n",
      "112/112 - 7s - loss: 1.1738e-06 - fn: 18.0000 - fp: 9293.0000 - tn: 218136.0000 - tp: 399.0000 - precision: 0.0412 - recall: 0.9568 - val_loss: 0.0950 - val_fn: 8.0000 - val_fp: 1754.0000 - val_tn: 55132.0000 - val_tp: 67.0000 - val_precision: 0.0368 - val_recall: 0.8933\n",
      "Epoch 12/30\n",
      "112/112 - 8s - loss: 1.9638e-06 - fn: 19.0000 - fp: 5780.0000 - tn: 221649.0000 - tp: 398.0000 - precision: 0.0644 - recall: 0.9544 - val_loss: 0.2758 - val_fn: 8.0000 - val_fp: 3894.0000 - val_tn: 52992.0000 - val_tp: 67.0000 - val_precision: 0.0169 - val_recall: 0.8933\n",
      "Epoch 13/30\n",
      "112/112 - 8s - loss: 1.2768e-06 - fn: 16.0000 - fp: 6168.0000 - tn: 221261.0000 - tp: 401.0000 - precision: 0.0610 - recall: 0.9616 - val_loss: 0.0681 - val_fn: 8.0000 - val_fp: 1380.0000 - val_tn: 55506.0000 - val_tp: 67.0000 - val_precision: 0.0463 - val_recall: 0.8933\n",
      "Epoch 14/30\n",
      "112/112 - 8s - loss: 9.2554e-07 - fn: 12.0000 - fp: 6039.0000 - tn: 221390.0000 - tp: 405.0000 - precision: 0.0628 - recall: 0.9712 - val_loss: 0.0822 - val_fn: 8.0000 - val_fp: 1545.0000 - val_tn: 55341.0000 - val_tp: 67.0000 - val_precision: 0.0416 - val_recall: 0.8933\n",
      "Epoch 15/30\n",
      "112/112 - 8s - loss: 5.1667e-07 - fn: 8.0000 - fp: 5390.0000 - tn: 222039.0000 - tp: 409.0000 - precision: 0.0705 - recall: 0.9808 - val_loss: 0.1316 - val_fn: 7.0000 - val_fp: 3381.0000 - val_tn: 53505.0000 - val_tp: 68.0000 - val_precision: 0.0197 - val_recall: 0.9067\n",
      "Epoch 16/30\n",
      "112/112 - 7s - loss: 4.0369e-07 - fn: 5.0000 - fp: 4843.0000 - tn: 222586.0000 - tp: 412.0000 - precision: 0.0784 - recall: 0.9880 - val_loss: 0.0422 - val_fn: 10.0000 - val_fp: 983.0000 - val_tn: 55903.0000 - val_tp: 65.0000 - val_precision: 0.0620 - val_recall: 0.8667\n",
      "Epoch 17/30\n",
      "112/112 - 7s - loss: 4.1614e-07 - fn: 6.0000 - fp: 5084.0000 - tn: 222345.0000 - tp: 411.0000 - precision: 0.0748 - recall: 0.9856 - val_loss: 0.0708 - val_fn: 8.0000 - val_fp: 1904.0000 - val_tn: 54982.0000 - val_tp: 67.0000 - val_precision: 0.0340 - val_recall: 0.8933\n",
      "Epoch 18/30\n",
      "112/112 - 10s - loss: 3.6532e-07 - fn: 5.0000 - fp: 3961.0000 - tn: 223468.0000 - tp: 412.0000 - precision: 0.0942 - recall: 0.9880 - val_loss: 0.1371 - val_fn: 9.0000 - val_fp: 2379.0000 - val_tn: 54507.0000 - val_tp: 66.0000 - val_precision: 0.0270 - val_recall: 0.8800\n",
      "Epoch 19/30\n",
      "112/112 - 8s - loss: 3.8391e-07 - fn: 6.0000 - fp: 4498.0000 - tn: 222931.0000 - tp: 411.0000 - precision: 0.0837 - recall: 0.9856 - val_loss: 0.0545 - val_fn: 9.0000 - val_fp: 1533.0000 - val_tn: 55353.0000 - val_tp: 66.0000 - val_precision: 0.0413 - val_recall: 0.8800\n",
      "Epoch 20/30\n",
      "112/112 - 7s - loss: 4.4746e-07 - fn: 4.0000 - fp: 4535.0000 - tn: 222894.0000 - tp: 413.0000 - precision: 0.0835 - recall: 0.9904 - val_loss: 0.0441 - val_fn: 10.0000 - val_fp: 1324.0000 - val_tn: 55562.0000 - val_tp: 65.0000 - val_precision: 0.0468 - val_recall: 0.8667\n",
      "Epoch 21/30\n",
      "112/112 - 9s - loss: 2.7242e-07 - fn: 2.0000 - fp: 2992.0000 - tn: 224437.0000 - tp: 415.0000 - precision: 0.1218 - recall: 0.9952 - val_loss: 0.0752 - val_fn: 9.0000 - val_fp: 1705.0000 - val_tn: 55181.0000 - val_tp: 66.0000 - val_precision: 0.0373 - val_recall: 0.8800\n",
      "Epoch 22/30\n",
      "112/112 - 9s - loss: 2.8513e-07 - fn: 3.0000 - fp: 2482.0000 - tn: 224947.0000 - tp: 414.0000 - precision: 0.1430 - recall: 0.9928 - val_loss: 0.0463 - val_fn: 10.0000 - val_fp: 904.0000 - val_tn: 55982.0000 - val_tp: 65.0000 - val_precision: 0.0671 - val_recall: 0.8667\n",
      "Epoch 23/30\n",
      "112/112 - 10s - loss: 4.4716e-07 - fn: 5.0000 - fp: 5232.0000 - tn: 222197.0000 - tp: 412.0000 - precision: 0.0730 - recall: 0.9880 - val_loss: 0.1298 - val_fn: 7.0000 - val_fp: 2573.0000 - val_tn: 54313.0000 - val_tp: 68.0000 - val_precision: 0.0257 - val_recall: 0.9067\n",
      "Epoch 24/30\n",
      "112/112 - 7s - loss: 7.9747e-07 - fn: 7.0000 - fp: 4500.0000 - tn: 222929.0000 - tp: 410.0000 - precision: 0.0835 - recall: 0.9832 - val_loss: 0.0934 - val_fn: 9.0000 - val_fp: 1379.0000 - val_tn: 55507.0000 - val_tp: 66.0000 - val_precision: 0.0457 - val_recall: 0.8800\n",
      "Epoch 25/30\n",
      "112/112 - 8s - loss: 4.0194e-07 - fn: 4.0000 - fp: 4900.0000 - tn: 222529.0000 - tp: 413.0000 - precision: 0.0777 - recall: 0.9904 - val_loss: 0.0712 - val_fn: 9.0000 - val_fp: 1650.0000 - val_tn: 55236.0000 - val_tp: 66.0000 - val_precision: 0.0385 - val_recall: 0.8800\n",
      "Epoch 26/30\n",
      "112/112 - 7s - loss: 3.1411e-07 - fn: 4.0000 - fp: 3113.0000 - tn: 224316.0000 - tp: 413.0000 - precision: 0.1171 - recall: 0.9904 - val_loss: 0.0844 - val_fn: 8.0000 - val_fp: 2004.0000 - val_tn: 54882.0000 - val_tp: 67.0000 - val_precision: 0.0324 - val_recall: 0.8933\n",
      "Epoch 27/30\n",
      "112/112 - 7s - loss: 4.2582e-07 - fn: 3.0000 - fp: 4948.0000 - tn: 222481.0000 - tp: 414.0000 - precision: 0.0772 - recall: 0.9928 - val_loss: 0.0616 - val_fn: 9.0000 - val_fp: 1364.0000 - val_tn: 55522.0000 - val_tp: 66.0000 - val_precision: 0.0462 - val_recall: 0.8800\n",
      "Epoch 28/30\n",
      "112/112 - 6s - loss: 3.0124e-07 - fn: 4.0000 - fp: 3729.0000 - tn: 223700.0000 - tp: 413.0000 - precision: 0.0997 - recall: 0.9904 - val_loss: 0.0565 - val_fn: 7.0000 - val_fp: 1501.0000 - val_tn: 55385.0000 - val_tp: 68.0000 - val_precision: 0.0433 - val_recall: 0.9067\n",
      "Epoch 29/30\n",
      "112/112 - 7s - loss: 1.9487e-07 - fn: 0.0000e+00 - fp: 2356.0000 - tn: 225073.0000 - tp: 417.0000 - precision: 0.1504 - recall: 1.0000 - val_loss: 0.0433 - val_fn: 8.0000 - val_fp: 955.0000 - val_tn: 55931.0000 - val_tp: 67.0000 - val_precision: 0.0656 - val_recall: 0.8933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "112/112 - 6s - loss: 1.6606e-07 - fn: 1.0000 - fp: 2201.0000 - tn: 225228.0000 - tp: 416.0000 - precision: 0.1590 - recall: 0.9976 - val_loss: 0.0382 - val_fn: 10.0000 - val_fp: 735.0000 - val_tn: 56151.0000 - val_tp: 65.0000 - val_precision: 0.0812 - val_recall: 0.8667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9fd3afbf40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = [\n",
    "    keras.metrics.FalseNegatives(name=\"fn\"),\n",
    "    keras.metrics.FalsePositives(name=\"fp\"),\n",
    "    keras.metrics.TrueNegatives(name=\"tn\"),\n",
    "    keras.metrics.TruePositives(name=\"tp\"),\n",
    "    keras.metrics.Precision(name=\"precision\"),\n",
    "    keras.metrics.Recall(name=\"recall\"),\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-2), loss=\"binary_crossentropy\", metrics=metrics\n",
    ")\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "model.fit(\n",
    "    train_feature_normal,\n",
    "    train_target,\n",
    "    batch_size=2048,\n",
    "    epochs=30,\n",
    "    verbose=2,\n",
    "    class_weight=class_weight,\n",
    "    validation_data=(val_features_normal, val_target)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "Looking at the training result (focusing on validation part): \n",
    "\n",
    "- FP: 735\n",
    "- FN: 10\n",
    "- TP: 65\n",
    "- TN: 56151\n",
    "\n",
    "This means that we correctly identified 65 numbers of fraudulant activities and we missed 10 of them. This cost us flagging 735 of legitimate activities as fraudulant which is a price we pay! Knowing that False Negatives are more important (wrongfully classify a fraudulant transaction as a legitimate one) this result is quite satisfactory. And, in the real world, they even put more weight on the positive cases which causes even more False Negatives!  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Stuff to Read (Optional):\n",
    "\n",
    "- Binary Cross Entropy: loss function for binary classification: \n",
    "https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\n",
    "\n",
    "- Adam optimization: original paper: https://arxiv.org/abs/1412.6980\n",
    "\n",
    "- Adam optimization: nice video explaning it: https://www.youtube.com/watch?v=JXQT_vxqwIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
