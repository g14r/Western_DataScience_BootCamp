{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDb Movie Reviews is a dataset containing about 50,000 reviews which are labeled as positive or negative (binary classification). The dataset is balanced (this means that the data contain even numbers of positive and negative reviews). We only consider polarized reviews (either very negative or very positive). \n",
    "\n",
    "- We want to make a model which is capbale of telling us if a review is positive or negative. The reviews are text messages, so that they need to become \"vectors\" as mentioned in deatils in lesson 3. \n",
    "- The length of each of these vectors should be the same (some review texts are shorter, some are longer. But, the input of the model should have the same size!) We can use Keras \"pad_sequences\" to ensure that all sequences in a list have the same length. Thus, we need to decide how many words from each movie review to pick (maxlen). \n",
    "- We are training a model in which the sequence of data matters. LSTM (long short term memory) algorithm is a neural network that takes care of the sequential data (text reviews). We are using Keras API to build our LSTM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import all the libraries we need\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "max_features = 20000  # We only consider the top 20k words\n",
    "maxlen = 200  # We only take the first 200 words of each movie review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "# Keras has its own built in datasets, one of them is the imdb data set which can be eaily loaded. \n",
    "# For convinece of the users, the imdb data is already being vecotrized and you can't see the review texts. \n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
    "    num_words=max_features)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using keras.preprocessing.sequence.pad_sequences we make sure that all the vectors have the same size\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 128)         2560000   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, None, 128)        98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,757,761\n",
      "Trainable params: 2,757,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now we build our model (using keras). At first we define the input. \n",
    "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "# Using word embeddings a dense representation of words and their relative meanings can be found. \n",
    "# Keras provides an embedding layer that works on integer inputs (our input is integer now).\n",
    "# The Embedding layer is initialized with random weights.\n",
    "# Embed each integer in a 128-dimensional vector.\n",
    "# see this: https://keras.io/api/layers/core_layers/embedding/#embedding\n",
    "x = layers.Embedding(max_features, 128)(inputs)\n",
    "# Add LSTM layers, we use a type of LSTM which is called Bidirectional \n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
    "# This is the output layer\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "# We build the model\n",
    "model = keras.Model(inputs, outputs)\n",
    "# summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - ETA: 0s - loss: 0.4000 - accuracy: 0.8186"
     ]
    }
   ],
   "source": [
    "# compile the model using Adam optimizaer, and the loss fucnstion is chosen to be \"binary_crossentropy\"\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# fir the model\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
